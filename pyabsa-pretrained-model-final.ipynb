{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune with PyABSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch==2.2.2 torchvision torchaudio --upgrade\n",
    "# %pip install transformers==4.39.3 --upgrade\n",
    "# %pip install peft==0.10.0 --upgrade\n",
    "# %pip install pyabsa --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyABSA imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyabsa import ABSADatasetList, DatasetItem, AspectPolarityClassification as APC, ModelSaveOption, DeviceTypeOption, ATEPCCheckpointManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure ATE (Aspect Term Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-27 15:55:37] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-04-27 15:55:37] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-04-27 15:55:37] (2.4.1.post1) Downloading checkpoint:english \n",
      "[2025-04-27 15:55:37] (2.4.1.post1) Notice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading checkpoint: 579MB [02:57,  3.27MB/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find zipped checkpoint: ./checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43.zip, unzipping\n",
      "Done.\n",
      "[2025-04-27 15:58:40] (2.4.1.post1) If the auto-downloading failed, please download it via browser: https://huggingface.co/spaces/yangheng/PyABSA/resolve/main/checkpoints/English/ATEPC/fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43.zip \n",
      "[2025-04-27 15:58:40] (2.4.1.post1) Load aspect extractor from checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\n",
      "[2025-04-27 15:58:40] (2.4.1.post1) config: checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\fast_lcf_atepc.config\n",
      "[2025-04-27 15:58:40] (2.4.1.post1) state_dict: checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\fast_lcf_atepc.state_dict\n",
      "[2025-04-27 15:58:40] (2.4.1.post1) model: None\n",
      "[2025-04-27 15:58:40] (2.4.1.post1) tokenizer: checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\fast_lcf_atepc.tokenizer\n",
      "[2025-04-27 15:58:40] (2.4.1.post1) Set Model Device: cpu\n",
      "[2025-04-27 15:58:40] (2.4.1.post1) Device Name: Unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\prediction\\aspect_extractor.py:593: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  lcf_cdm_vec = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-27 15:58:48] (2.4.1.post1) The results of aspect term extraction have been saved in d:\\FCAIH - Materials\\Natural Language Understanding\\Project\\jakarta_research_sameval_absa\\notebooks\\Aspect Term Extraction and Polarity Classification.FAST_LCF_ATEPC.result.json\n",
      "[2025-04-27 15:58:48] (2.4.1.post1) Example 0: The <food:Positive Confidence:0.9952> is good but the <service:Negative Confidence:0.9973> is bad .\n",
      "[{'sentence': 'The food is good but the service is bad .', 'IOB': ['O', 'B-ASP', 'O', 'O', 'O', 'O', 'B-ASP', 'O', 'O', 'O'], 'tokens': ['The', 'food', 'is', 'good', 'but', 'the', 'service', 'is', 'bad', '.'], 'aspect': ['food', 'service'], 'position': [[1], [6]], 'sentiment': ['Positive', 'Negative'], 'probs': [[0.004458647221326828, 0.0003178288752678782, 0.9952235817909241], [0.997255265712738, 0.0016527267871424556, 0.0010920086642727256]], 'confidence': [0.9952, 0.9973]}]\n"
     ]
    }
   ],
   "source": [
    "aspect_extractor = ATEPCCheckpointManager.get_aspect_extractor(checkpoint=\"english\")\n",
    "\n",
    "inference_source = [\n",
    "    'The food is good but the service is bad.',\n",
    "]\n",
    "\n",
    "atepc_result = aspect_extractor.extract_aspect(inference_source=inference_source)\n",
    "print(atepc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping ATE output with [ASP] tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The [ASP] food [ASP] is good but the service is bad .', 'The food is good but the [ASP] service [ASP] is bad .']\n"
     ]
    }
   ],
   "source": [
    "def insert_asp_tokens(sentence, aspect):\n",
    "    \"\"\"Insert [ASP] tokens around the aspect term in the sentence.\"\"\"\n",
    "    return sentence.replace(aspect, f\"[ASP] {aspect} [ASP]\", 1)\n",
    "\n",
    "apc_inputs = []\n",
    "for result in atepc_result:\n",
    "    sentence = result['sentence']\n",
    "    for aspect in result['aspect']:\n",
    "        formatted_sentence = insert_asp_tokens(sentence, aspect)\n",
    "        apc_inputs.append(formatted_sentence)\n",
    "\n",
    "print(apc_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = APC.APCConfigManager.get_apc_config_english()\n",
    "\n",
    "dataset = ABSADatasetList.Laptop14\n",
    "\n",
    "# my_dataset = DatasetItem(\"my_dataset\", [\"my_dataset1\", \"my_dataset2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APC (Aspect Polarity Classification) configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-27T01:40:44.213Z",
     "iopub.execute_input": "2025-04-27T01:40:34.247730Z",
     "iopub.status.busy": "2025-04-27T01:40:34.247111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-27 14:42:05] (2.4.1.post1) Set Model Device: cpu\n",
      "[2025-04-27 14:42:05] (2.4.1.post1) Device Name: Unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-27 14:42:05,344 INFO: PyABSA version: 2.4.1.post1\n",
      "2025-04-27 14:42:05,346 INFO: Transformers version: 4.39.3\n",
      "2025-04-27 14:42:05,347 INFO: Torch version: 2.2.2+cpu+cudaNone\n",
      "2025-04-27 14:42:05,348 INFO: Device: Unknown\n",
      "2025-04-27 14:42:05,359 INFO: Searching dataset 113.Laptop14 in local disk\n",
      "2025-04-27 14:42:05,498 INFO: You can set load_aug=True in a trainer to augment your dataset (English only yet) and improve performance.\n",
      "2025-04-27 14:42:05,499 INFO: Please use a new folder to perform new text augment if the former augment in integrated_datasets\\apc_datasets\\110.SemEval\\113.laptop14 errored unexpectedly\n",
      "[2025-04-27 14:42:05] (2.4.1.post1) Loading dataset cache: fast_lsa_t_v2.Laptop14.dataset.20b920d5f04090745bb62188f5b07715de5535acb34814f724371a92053a0b34.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-27 14:42:14,888 INFO: Model Architecture:\n",
      " APCEnsembler(\n",
      "  (models): ModuleList(\n",
      "    (0): FAST_LSA_T_V2(\n",
      "      (bert4global): DebertaV2Model(\n",
      "        (embeddings): DebertaV2Embeddings(\n",
      "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "          (dropout): StableDropout()\n",
      "        )\n",
      "        (encoder): DebertaV2Encoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x DebertaV2Layer(\n",
      "              (attention): DebertaV2Attention(\n",
      "                (self): DisentangledSelfAttention(\n",
      "                  (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (pos_dropout): StableDropout()\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "                (output): DebertaV2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "              )\n",
      "              (intermediate): DebertaV2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DebertaV2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (rel_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (post_encoder): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (post_encoder_): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (bert_pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (CDW_LSA): LSA(\n",
      "        (encoder): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_left): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_right): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (linear_window_3h): Linear(in_features=2304, out_features=768, bias=True)\n",
      "        (linear_window_2h): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      )\n",
      "      (post_linear): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      (dense): Linear(in_features=768, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (bert): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (dense): Linear(in_features=3, out_features=3, bias=True)\n",
      ")\n",
      "2025-04-27 14:42:14,892 INFO: ABSADatasetsVersion:None\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,893 INFO: MV:<metric_visualizer.metric_visualizer.MetricVisualizer object at 0x000001E0F6225820>\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,895 INFO: PyABSAVersion:2.4.1.post1\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,897 INFO: SRD:3\t-->\tCalling Count:5932\n",
      "2025-04-27 14:42:14,899 INFO: TorchVersion:2.2.2+cpu+cudaNone\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,900 INFO: TransformersVersion:4.39.3\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,901 INFO: auto_device:True\t-->\tCalling Count:3\n",
      "2025-04-27 14:42:14,904 INFO: batch_size:16\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,905 INFO: cache_dataset:True\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,906 INFO: checkpoint_save_mode:1\t-->\tCalling Count:4\n",
      "2025-04-27 14:42:14,908 INFO: cross_validate_fold:-1\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,909 INFO: dataset_file:{'train': ['integrated_datasets\\\\apc_datasets\\\\110.SemEval\\\\113.laptop14\\\\Laptops_Train.xml.seg'], 'test': ['integrated_datasets\\\\apc_datasets\\\\110.SemEval\\\\113.laptop14\\\\Laptops_Test_Gold.xml.seg'], 'valid': []}\t-->\tCalling Count:15\n",
      "2025-04-27 14:42:14,911 INFO: dataset_name:Laptop14\t-->\tCalling Count:3\n",
      "2025-04-27 14:42:14,912 INFO: dca_layer:3\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,914 INFO: dca_p:1\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,915 INFO: deep_ensemble:False\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,917 INFO: device:cpu\t-->\tCalling Count:3\n",
      "2025-04-27 14:42:14,918 INFO: device_name:Unknown\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,920 INFO: dlcf_a:2\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,921 INFO: dropout:0.5\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,921 INFO: dynamic_truncate:True\t-->\tCalling Count:5932\n",
      "2025-04-27 14:42:14,922 INFO: embed_dim:768\t-->\tCalling Count:7\n",
      "2025-04-27 14:42:14,923 INFO: eta:1\t-->\tCalling Count:2\n",
      "2025-04-27 14:42:14,924 INFO: eta_lr:0.1\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,926 INFO: evaluate_begin:0\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,930 INFO: from_checkpoint:english\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,934 INFO: hidden_dim:768\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,938 INFO: index_to_label:{0: 'Negative', 1: 'Neutral', 2: 'Positive'}\t-->\tCalling Count:2\n",
      "2025-04-27 14:42:14,940 INFO: inference_model:None\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,942 INFO: initializer:xavier_uniform_\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,943 INFO: inputs_cols:['lcf_cdm_vec', 'lcf_cdw_vec', 'left_lcf_cdm_vec', 'left_lcf_cdw_vec', 'right_lcf_cdm_vec', 'right_lcf_cdw_vec', 'spc_mask_vec', 'text_indices']\t-->\tCalling Count:44495\n",
      "2025-04-27 14:42:14,945 INFO: l2reg:1e-06\t-->\tCalling Count:2\n",
      "2025-04-27 14:42:14,946 INFO: label_to_index:{'Negative': 0, 'Neutral': 1, 'Positive': 2}\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,947 INFO: lcf:cdw\t-->\tCalling Count:3\n",
      "2025-04-27 14:42:14,947 INFO: learning_rate:2e-05\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,948 INFO: load_aug:False\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,949 INFO: log_step:5\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,954 INFO: logger:<Logger fast_lsa_t_v2 (INFO)>\t-->\tCalling Count:14\n",
      "2025-04-27 14:42:14,970 INFO: lsa:False\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,979 INFO: max_seq_len:80\t-->\tCalling Count:35592\n",
      "2025-04-27 14:42:14,982 INFO: model:<class 'pyabsa.tasks.AspectPolarityClassification.models.__lcf__.fast_lsa_t_v2.FAST_LSA_T_V2'>\t-->\tCalling Count:6\n",
      "2025-04-27 14:42:14,989 INFO: model_name:fast_lsa_t_v2\t-->\tCalling Count:5934\n",
      "2025-04-27 14:42:14,992 INFO: model_path_to_save:checkpoints\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,994 INFO: num_epoch:1\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:14,996 INFO: optimizer:adamw\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,997 INFO: output_dim:3\t-->\tCalling Count:3\n",
      "2025-04-27 14:42:14,998 INFO: overwrite_cache:False\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:14,999 INFO: path_to_save:None\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:15,000 INFO: patience:99999\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:15,002 INFO: pretrained_bert:yangheng/deberta-v3-base-absa-v1.1\t-->\tCalling Count:5\n",
      "2025-04-27 14:42:15,003 INFO: save_mode:1\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:15,005 INFO: seed:52\t-->\tCalling Count:7\n",
      "2025-04-27 14:42:15,005 INFO: sigma:0.3\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:15,006 INFO: similarity_threshold:1\t-->\tCalling Count:2\n",
      "2025-04-27 14:42:15,008 INFO: spacy_model:en_core_web_sm\t-->\tCalling Count:7\n",
      "2025-04-27 14:42:15,009 INFO: srd_alignment:True\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:15,010 INFO: task_code:APC\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:15,010 INFO: task_name:Aspect-based Sentiment Classification\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:15,011 INFO: tokenizer:DebertaV2TokenizerFast(name_or_path='yangheng/deberta-v3-base-absa-v1.1', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t128000: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:15,013 INFO: use_amp:False\t-->\tCalling Count:1\n",
      "2025-04-27 14:42:15,013 INFO: use_bert_spc:True\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:15,016 INFO: use_syntax_based_SRD:False\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:15,017 INFO: warmup_step:-1\t-->\tCalling Count:0\n",
      "2025-04-27 14:42:15,024 INFO: window:lr\t-->\tCalling Count:0\n",
      "[2025-04-27 14:42:15] (2.4.1.post1) ********** Available APC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-04-27 14:42:15] (2.4.1.post1) ********** Available APC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-04-27 14:42:15] (2.4.1.post1) Downloading checkpoint:english \n",
      "[2025-04-27 14:42:15] (2.4.1.post1) Notice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\n",
      "[2025-04-27 14:42:15] (2.4.1.post1) Checkpoint already downloaded, skip\n",
      "2025-04-27 14:42:15,442 INFO: Checkpoint downloaded at: checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyabsa\\framework\\instructor_class\\instructor_template.py:430: ResourceWarning: unclosed file <_io.BufferedReader name='checkpoints\\\\APC_ENGLISH_CHECKPOINT\\\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\\\fast_lsa_t_v2.config'>\n",
      "  config = pickle.load(open(config_path[0], \"rb\"))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-27 14:42:16,922 INFO: Resume trainer from Checkpoint: checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81!\n",
      "2025-04-27 14:42:16,928 INFO: ***** Running training for Aspect-based Sentiment Classification *****\n",
      "2025-04-27 14:42:16,929 INFO: Training set examples = 2328\n",
      "2025-04-27 14:42:16,931 INFO: Test set examples = 638\n",
      "2025-04-27 14:42:16,933 INFO: Total params = 197414417, Trainable params = 197414417, Non-trainable params = 0\n",
      "2025-04-27 14:42:16,933 INFO: Batch size = 16\n",
      "2025-04-27 14:42:16,934 INFO: Num steps = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Loss:0:   0%|          | 0/146 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-27 14:42:19] (2.4.1.post1) reset eta1 to: 0.767673671245575\n",
      "[2025-04-27 14:42:19] (2.4.1.post1) reset eta2 to: 0.7462112307548523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Smooth Loss: 0.1318: 100%|██████████| 146/146 [46:44<00:00, 19.21s/it, Dev Acc:80.88(max:84.01) Dev F1:76.13(max:80.97)] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:29:01,649 INFO: \n",
      "--------------------------------------------------------------------------- Raw Metric Records ---------------------------------------------------------------------------\n",
      "╒════════════════════════════╤═══════════════════════════════════════════════════════════╤═════════════════════╤═══════════╤══════════╤═══════╤═══════╤═════════╤═════════╕\n",
      "│ Metric                     │ Trial                                                     │ Values              │  Average  │  Median  │  Std  │  IQR  │   Min   │   Max   │\n",
      "╞════════════════════════════╪═══════════════════════════════════════════════════════════╪═════════════════════╪═══════════╪══════════╪═══════╪═══════╪═════════╪═════════╡\n",
      "│ Max-Test-Acc w/o Valid Set │ fast_lsa_t_v2-Laptop14-yangheng/deberta-v3-base-absa-v1.1 │ [84.01253918495298] │  84.0125  │ 84.0125  │   0   │   0   │ 84.0125 │ 84.0125 │\n",
      "├────────────────────────────┼───────────────────────────────────────────────────────────┼─────────────────────┼───────────┼──────────┼───────┼───────┼─────────┼─────────┤\n",
      "│ Max-Test-F1 w/o Valid Set  │ fast_lsa_t_v2-Laptop14-yangheng/deberta-v3-base-absa-v1.1 │ [80.97050328593953] │  80.9705  │ 80.9705  │   0   │   0   │ 80.9705 │ 80.9705 │\n",
      "╘════════════════════════════╧═══════════════════════════════════════════════════════════╧═════════════════════╧═══════════╧══════════╧═══════╧═══════╧═════════╧═════════╛\n",
      "------------------------------------------------------------- https://github.com/yangheng95/metric_visualizer -------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyabsa\\framework\\trainer_class\\trainer_template.py:250: ResourceWarning: unclosed file <_io.TextIOWrapper name='d:\\\\FCAIH - Materials\\\\Natural Language Understanding\\\\Project\\\\jakarta_research_sameval_absa\\\\notebooks\\\\logs\\\\fast_lsa_t_v2_20250427 144042\\\\trainer.log' mode='a' encoding='utf8'>\n",
      "  self.config.logger.removeHandler(self.config.logger.handlers[0])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "config = APC.APCConfigManager.get_apc_config_english()\n",
    "config.num_epoch = 1\n",
    "config.model = APC.APCModelList.FAST_LSA_T_V2\n",
    "\n",
    "# dataset='D:/FCAIH - Materials/Natural Language Understanding/Project/jakarta_research_sameval_absa/datasets/apc_datasets/101.restaurant'\n",
    "\n",
    "trainer = APC.APCTrainer(\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    from_checkpoint=\"english\",\n",
    "    # if you want to resume training from our pretrained checkpoints, you can pass the checkpoint name here\n",
    "    auto_device=DeviceTypeOption.AUTO,\n",
    "    path_to_save=None,  # set a path to save checkpoints, if it is None, save checkpoints at 'checkpoints' folder\n",
    "    checkpoint_save_mode=ModelSaveOption.SAVE_MODEL_STATE_DICT,\n",
    "    load_aug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-27 15:29:14] (2.4.1.post1) Load sentiment classifier from checkpoints/fast_lsa_t_v2_Laptop14_acc_84.01_f1_80.97/\n",
      "[2025-04-27 15:29:14] (2.4.1.post1) config: checkpoints/fast_lsa_t_v2_Laptop14_acc_84.01_f1_80.97/fast_lsa_t_v2.config\n",
      "[2025-04-27 15:29:14] (2.4.1.post1) state_dict: checkpoints/fast_lsa_t_v2_Laptop14_acc_84.01_f1_80.97/fast_lsa_t_v2.state_dict\n",
      "[2025-04-27 15:29:14] (2.4.1.post1) model: None\n",
      "[2025-04-27 15:29:14] (2.4.1.post1) tokenizer: checkpoints/fast_lsa_t_v2_Laptop14_acc_84.01_f1_80.97/fast_lsa_t_v2.tokenizer\n",
      "[2025-04-27 15:29:14] (2.4.1.post1) Set Model Device: cpu\n",
      "[2025-04-27 15:29:14] (2.4.1.post1) Device Name: Unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-27 15:29:24] (2.4.1.post1) Please specify the task code, e.g. from pyabsa import TaskCodeOption\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) ********** Available APC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) ********** Available APC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) Downloading checkpoint:english \n",
      "[2025-04-27 15:29:26] (2.4.1.post1) Notice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) Checkpoint already downloaded, skip\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) Load sentiment classifier from checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) config: checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\fast_lsa_t_v2.config\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) state_dict: checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\fast_lsa_t_v2.state_dict\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) model: None\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) tokenizer: checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\fast_lsa_t_v2.tokenizer\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) Set Model Device: cpu\n",
      "[2025-04-27 15:29:26] (2.4.1.post1) Device Name: Unknown\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bb4b4948874b6f83f93a95b3b7aeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d90beb4b7cb4f7190e5e4e6a32e3942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyabsa.tasks.AspectPolarityClassification import SentimentClassifier\n",
    "\n",
    "sentiment_classifier = trainer.load_trained_model()\n",
    "assert isinstance(sentiment_classifier, SentimentClassifier)\n",
    "\n",
    "from pyabsa import available_checkpoints\n",
    "\n",
    "ckpts = available_checkpoints()\n",
    "\n",
    "sentiment_classifier = APC.SentimentClassifier(\n",
    "    checkpoint=\"english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-27 16:12:56] (2.4.1.post1) [ASP] tag is detected, please use [B-ASP] and [E-ASP] to annotate aspect terms.\n",
      "[2025-04-27 16:12:56] (2.4.1.post1) [ASP] tag is detected, please use [B-ASP] and [E-ASP] to annotate aspect terms.\n",
      "[2025-04-27 16:12:57] (2.4.1.post1) Example 0: The < food :Positive(confidence:0.998, ref:-100)> is good but the< service :Negative(confidence:0.988, ref:-100)>is bad .\n"
     ]
    }
   ],
   "source": [
    "apc_output = sentiment_classifier.predict(\n",
    "    text=apc_inputs,\n",
    "    print_result=True,\n",
    "    ignore_error=True,\n",
    "    eval_batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: The  food  is good but the service is bad .\n",
      "\n",
      "- Aspect: \"food\" | Sentiment: Positive | Confidence: 99.81%\n",
      "- Aspect: \"service\" | Sentiment: Negative | Confidence: 98.82%\n"
     ]
    }
   ],
   "source": [
    "def format_atepc_output(results):\n",
    "    for item in results:\n",
    "        print(f\"\\nSentence: {item['text'].strip()}\\n\")\n",
    "        for aspect, sentiment, confidence in zip(item['aspect'], item['sentiment'], item['confidence']):\n",
    "            aspect_clean = aspect.strip()\n",
    "            confidence_percent = round(confidence * 100, 2)\n",
    "            print(f\"- Aspect: \\\"{aspect_clean}\\\" | Sentiment: {sentiment} | Confidence: {confidence_percent}%\")\n",
    "\n",
    "format_atepc_output(apc_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The  food  is good but the service is bad .\n",
      "\n",
      "Output: (food: \"positive\", service: \"negative\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_atepc_output_custom(results):\n",
    "    for item in results:\n",
    "        aspects_info = []\n",
    "        for aspect, sentiment in zip(item['aspect'], item['sentiment']):\n",
    "            aspect_clean = aspect.strip()\n",
    "            sentiment_clean = sentiment.strip()\n",
    "            aspects_info.append(f'{aspect_clean}: \"{sentiment_clean.lower()}\"')\n",
    "        \n",
    "        aspects_str = ', '.join(aspects_info)\n",
    "        print(f\"Input: {item['text'].strip()}\\n\")\n",
    "        print(f\"Output: ({aspects_str})\\n\")\n",
    "\n",
    "format_atepc_output_custom(apc_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying fine-tuing with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "from pyabsa.tasks.AspectPolarityClassification import SentimentClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained model (Again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-27 18:33:24] (2.4.1.post1) Please specify the task code, e.g. from pyabsa import TaskCodeOption\n",
      "[2025-04-27 18:33:24] (2.4.1.post1) ********** Available APC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-04-27 18:33:24] (2.4.1.post1) ********** Available APC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-04-27 18:33:24] (2.4.1.post1) Downloading checkpoint:english \n",
      "[2025-04-27 18:33:24] (2.4.1.post1) Notice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\n",
      "[2025-04-27 18:33:24] (2.4.1.post1) Checkpoint already downloaded, skip\n",
      "FindFile Warning --> multiple targets ['checkpoints\\\\APC_ENGLISH_CHECKPOINT\\\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\\\fast_lsa_t_v2.config', 'checkpoints\\\\ATEPC_ENGLISH_CHECKPOINT\\\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\\\fast_lcf_atepc.config'] found, only return the shortest path: <checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\fast_lsa_t_v2.config>\n",
      "FindFile Warning --> multiple targets ['checkpoints\\\\APC_ENGLISH_CHECKPOINT\\\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\\\fast_lsa_t_v2.config', 'checkpoints\\\\ATEPC_ENGLISH_CHECKPOINT\\\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\\\fast_lcf_atepc.config'] found, only return the shortest path: <checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\fast_lsa_t_v2.config>\n",
      "[2025-04-27 18:33:25] (2.4.1.post1) Load sentiment classifier from checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\n",
      "[2025-04-27 18:33:25] (2.4.1.post1) config: checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\fast_lsa_t_v2.config\n",
      "[2025-04-27 18:33:25] (2.4.1.post1) state_dict: checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\fast_lsa_t_v2.state_dict\n",
      "[2025-04-27 18:33:25] (2.4.1.post1) model: None\n",
      "[2025-04-27 18:33:25] (2.4.1.post1) tokenizer: checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\fast_lsa_t_v2.tokenizer\n",
      "[2025-04-27 18:33:25] (2.4.1.post1) Set Model Device: cpu\n",
      "[2025-04-27 18:33:25] (2.4.1.post1) Device Name: Unknown\n"
     ]
    }
   ],
   "source": [
    "ckpts = available_checkpoints()\n",
    "\n",
    "sentiment_classifier = APC.SentimentClassifier(\n",
    "    checkpoint=\"english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LoRA layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_layer, r=8):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.r = r\n",
    "        self.lora_A = nn.Linear(original_layer.in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, original_layer.out_features, bias=False)\n",
    "        # Initialize LoRA weights\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.original_layer(x) + self.lora_B(self.lora_A(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding target layer in model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCEnsembler(\n",
      "  (models): ModuleList(\n",
      "    (0): FAST_LSA_T_V2(\n",
      "      (bert4global): DebertaV2Model(\n",
      "        (embeddings): DebertaV2Embeddings(\n",
      "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "          (dropout): StableDropout()\n",
      "        )\n",
      "        (encoder): DebertaV2Encoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x DebertaV2Layer(\n",
      "              (attention): DebertaV2Attention(\n",
      "                (self): DisentangledSelfAttention(\n",
      "                  (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (pos_dropout): StableDropout()\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "                (output): DebertaV2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "              )\n",
      "              (intermediate): DebertaV2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DebertaV2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (rel_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "      (post_encoder): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (post_encoder_): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (bert_pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (CDW_LSA): LSA(\n",
      "        (encoder): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_left): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_right): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (linear_window_3h): Linear(in_features=2304, out_features=768, bias=True)\n",
      "        (linear_window_2h): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      )\n",
      "      (post_linear): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      (dense): Linear(in_features=768, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (bert): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (dense): Linear(in_features=3, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_model = sentiment_classifier\n",
    "print(lora_model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the attention layers for LoRA application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in lora_model.model.models[0].bert4global.encoder.layer:\n",
    "    # Apply LoRA to each attention projection (query_proj, key_proj, value_proj)\n",
    "    layer.attention.self.query_proj = LoRALayer(layer.attention.self.query_proj, r=8)\n",
    "    layer.attention.self.key_proj = LoRALayer(layer.attention.self.key_proj, r=8)\n",
    "    layer.attention.self.value_proj = LoRALayer(layer.attention.self.value_proj, r=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding LoRA layer to target layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.model.dense = LoRALayer(lora_model.model.dense, r=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check new model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APCEnsembler(\n",
      "  (models): ModuleList(\n",
      "    (0): FAST_LSA_T_V2(\n",
      "      (bert4global): DebertaV2Model(\n",
      "        (embeddings): DebertaV2Embeddings(\n",
      "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "          (dropout): StableDropout()\n",
      "        )\n",
      "        (encoder): DebertaV2Encoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x DebertaV2Layer(\n",
      "              (attention): DebertaV2Attention(\n",
      "                (self): DisentangledSelfAttention(\n",
      "                  (query_proj): LoRALayer(\n",
      "                    (original_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (key_proj): LoRALayer(\n",
      "                    (original_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (value_proj): LoRALayer(\n",
      "                    (original_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (pos_dropout): StableDropout()\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "                (output): DebertaV2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "              )\n",
      "              (intermediate): DebertaV2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DebertaV2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (rel_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "      (post_encoder): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (post_encoder_): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (bert_pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (CDW_LSA): LSA(\n",
      "        (encoder): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_left): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_right): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (linear_window_3h): Linear(in_features=2304, out_features=768, bias=True)\n",
      "        (linear_window_2h): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      )\n",
      "      (post_linear): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      (dense): Linear(in_features=768, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (bert): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): LoRALayer(\n",
      "                (original_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
      "                (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
      "              )\n",
      "              (key_proj): LoRALayer(\n",
      "                (original_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
      "                (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
      "              )\n",
      "              (value_proj): LoRALayer(\n",
      "                (original_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
      "                (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
      "              )\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (dense): LoRALayer(\n",
      "    (original_layer): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (lora_A): Linear(in_features=3, out_features=8, bias=False)\n",
      "    (lora_B): Linear(in_features=8, out_features=3, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-27 18:39:28] (2.4.1.post1) Set Model Device: cpu\n",
      "[2025-04-27 18:39:28] (2.4.1.post1) Device Name: Unknown\n",
      "2025-04-27 18:39:29,144 INFO: PyABSA version: 2.4.1.post1\n",
      "2025-04-27 18:39:29,146 INFO: Transformers version: 4.39.3\n",
      "2025-04-27 18:39:29,147 INFO: Torch version: 2.2.2+cpu+cudaNone\n",
      "2025-04-27 18:39:29,148 INFO: Device: Unknown\n",
      "2025-04-27 18:39:29,220 INFO: Searching dataset 113.Laptop14 in local disk\n",
      "2025-04-27 18:39:29,337 INFO: You can set load_aug=True in a trainer to augment your dataset (English only yet) and improve performance.\n",
      "2025-04-27 18:39:29,338 INFO: Please use a new folder to perform new text augment if the former augment in integrated_datasets\\apc_datasets\\110.SemEval\\113.laptop14 errored unexpectedly\n",
      "[2025-04-27 18:39:29] (2.4.1.post1) Loading dataset cache: fast_lsa_t_v2.Laptop14.dataset.cf68cf1eebd128ab3fee3d23cf6ac43daddf54975a199e4e32e60f650e112f28.cache\n",
      "2025-04-27 18:39:38,961 INFO: Model Architecture:\n",
      " APCEnsembler(\n",
      "  (models): ModuleList(\n",
      "    (0): FAST_LSA_T_V2(\n",
      "      (bert4global): DebertaV2Model(\n",
      "        (embeddings): DebertaV2Embeddings(\n",
      "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "          (dropout): StableDropout()\n",
      "        )\n",
      "        (encoder): DebertaV2Encoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x DebertaV2Layer(\n",
      "              (attention): DebertaV2Attention(\n",
      "                (self): DisentangledSelfAttention(\n",
      "                  (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (pos_dropout): StableDropout()\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "                (output): DebertaV2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "              )\n",
      "              (intermediate): DebertaV2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DebertaV2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (rel_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (post_encoder): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (post_encoder_): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (bert_pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (CDW_LSA): LSA(\n",
      "        (encoder): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_left): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_right): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (linear_window_3h): Linear(in_features=2304, out_features=768, bias=True)\n",
      "        (linear_window_2h): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      )\n",
      "      (post_linear): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      (dense): Linear(in_features=768, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (bert): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (dense): Linear(in_features=3, out_features=3, bias=True)\n",
      ")\n",
      "2025-04-27 18:39:38,964 INFO: ABSADatasetsVersion:None\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:38,967 INFO: MV:<metric_visualizer.metric_visualizer.MetricVisualizer object at 0x000001E0F6C353A0>\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:38,969 INFO: PyABSAVersion:2.4.1.post1\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:38,970 INFO: SRD:3\t-->\tCalling Count:5932\n",
      "2025-04-27 18:39:38,972 INFO: TorchVersion:2.2.2+cpu+cudaNone\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:38,974 INFO: TransformersVersion:4.39.3\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:38,975 INFO: auto_device:cpu\t-->\tCalling Count:3\n",
      "2025-04-27 18:39:38,976 INFO: batch_size:16\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:38,977 INFO: cache_dataset:True\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:38,978 INFO: checkpoint_save_mode:0\t-->\tCalling Count:3\n",
      "2025-04-27 18:39:38,980 INFO: cross_validate_fold:-1\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:38,982 INFO: dataset_file:{'train': ['integrated_datasets\\\\apc_datasets\\\\110.SemEval\\\\113.laptop14\\\\Laptops_Train.xml.seg'], 'test': ['integrated_datasets\\\\apc_datasets\\\\110.SemEval\\\\113.laptop14\\\\Laptops_Test_Gold.xml.seg'], 'valid': []}\t-->\tCalling Count:14\n",
      "2025-04-27 18:39:38,983 INFO: dataset_name:Laptop14\t-->\tCalling Count:3\n",
      "2025-04-27 18:39:38,985 INFO: dca_layer:3\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:38,988 INFO: dca_p:1\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:38,989 INFO: deep_ensemble:False\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:38,990 INFO: device:cpu\t-->\tCalling Count:3\n",
      "2025-04-27 18:39:38,991 INFO: device_name:Unknown\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:38,993 INFO: dlcf_a:2\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:38,994 INFO: dropout:0.5\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:38,995 INFO: dynamic_truncate:True\t-->\tCalling Count:5932\n",
      "2025-04-27 18:39:38,997 INFO: embed_dim:768\t-->\tCalling Count:7\n",
      "2025-04-27 18:39:38,998 INFO: eta:1\t-->\tCalling Count:2\n",
      "2025-04-27 18:39:38,999 INFO: eta_lr:0.1\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:39,001 INFO: evaluate_begin:0\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,003 INFO: from_checkpoint:None\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,005 INFO: hidden_dim:768\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,009 INFO: index_to_label:{0: 'Negative', 1: 'Neutral', 2: 'Positive'}\t-->\tCalling Count:2\n",
      "2025-04-27 18:39:39,011 INFO: inference_model:None\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,013 INFO: initializer:xavier_uniform_\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,014 INFO: inputs_cols:['lcf_cdm_vec', 'lcf_cdw_vec', 'left_lcf_cdm_vec', 'left_lcf_cdw_vec', 'right_lcf_cdm_vec', 'right_lcf_cdw_vec', 'spc_mask_vec', 'text_indices']\t-->\tCalling Count:44495\n",
      "2025-04-27 18:39:39,016 INFO: l2reg:1e-06\t-->\tCalling Count:2\n",
      "2025-04-27 18:39:39,019 INFO: label_to_index:{'Negative': 0, 'Neutral': 1, 'Positive': 2}\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,023 INFO: lcf:cdw\t-->\tCalling Count:3\n",
      "2025-04-27 18:39:39,024 INFO: learning_rate:2e-05\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:39,026 INFO: load_aug:False\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:39,027 INFO: log_step:5\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,029 INFO: logger:<Logger fast_lsa_t_v2 (INFO)>\t-->\tCalling Count:14\n",
      "2025-04-27 18:39:39,031 INFO: lsa:False\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,033 INFO: max_seq_len:80\t-->\tCalling Count:35592\n",
      "2025-04-27 18:39:39,034 INFO: model:<class 'pyabsa.tasks.AspectPolarityClassification.models.__lcf__.fast_lsa_t_v2.FAST_LSA_T_V2'>\t-->\tCalling Count:6\n",
      "2025-04-27 18:39:39,036 INFO: model_name:fast_lsa_t_v2\t-->\tCalling Count:5934\n",
      "2025-04-27 18:39:39,037 INFO: model_path_to_save:None\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,039 INFO: num_epoch:1\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,040 INFO: optimizer:adamw\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:39,042 INFO: output_dim:3\t-->\tCalling Count:3\n",
      "2025-04-27 18:39:39,043 INFO: overwrite_cache:False\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:39,044 INFO: path_to_save:None\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,046 INFO: patience:99999\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,047 INFO: pretrained_bert:yangheng/deberta-v3-base-absa-v1.1\t-->\tCalling Count:5\n",
      "2025-04-27 18:39:39,050 INFO: save_mode:0\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,051 INFO: seed:52\t-->\tCalling Count:7\n",
      "2025-04-27 18:39:39,052 INFO: sigma:0.3\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,054 INFO: similarity_threshold:1\t-->\tCalling Count:2\n",
      "2025-04-27 18:39:39,055 INFO: spacy_model:en_core_web_sm\t-->\tCalling Count:3\n",
      "2025-04-27 18:39:39,057 INFO: srd_alignment:True\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,058 INFO: task_code:APC\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:39,060 INFO: task_name:Aspect-based Sentiment Classification\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,062 INFO: tokenizer:DebertaV2TokenizerFast(name_or_path='yangheng/deberta-v3-base-absa-v1.1', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t128000: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,064 INFO: use_amp:False\t-->\tCalling Count:1\n",
      "2025-04-27 18:39:39,065 INFO: use_bert_spc:True\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,066 INFO: use_syntax_based_SRD:False\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,067 INFO: warmup_step:-1\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,070 INFO: window:lr\t-->\tCalling Count:0\n",
      "2025-04-27 18:39:39,077 INFO: ***** Running training for Aspect-based Sentiment Classification *****\n",
      "2025-04-27 18:39:39,103 INFO: Training set examples = 2328\n",
      "2025-04-27 18:39:39,114 INFO: Test set examples = 638\n",
      "2025-04-27 18:39:39,115 INFO: Total params = 197414417, Trainable params = 197414417, Non-trainable params = 0\n",
      "2025-04-27 18:39:39,116 INFO: Batch size = 16\n",
      "2025-04-27 18:39:39,116 INFO: Num steps = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Smooth Loss: 0.2367: 100%|██████████| 146/146 [47:35<00:00, 19.56s/it, Dev Acc:81.97(max:83.86) Dev F1:78.23(max:80.47)] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-27 19:27:14,569 INFO: \n",
      "--------------------------------------------------------------------------- Raw Metric Records ---------------------------------------------------------------------------\n",
      "╒════════════════════════════╤═══════════════════════════════════════════════════════════╤═════════════════════╤═══════════╤══════════╤═══════╤═══════╤═════════╤═════════╕\n",
      "│ Metric                     │ Trial                                                     │ Values              │  Average  │  Median  │  Std  │  IQR  │   Min   │   Max   │\n",
      "╞════════════════════════════╪═══════════════════════════════════════════════════════════╪═════════════════════╪═══════════╪══════════╪═══════╪═══════╪═════════╪═════════╡\n",
      "│ Max-Test-Acc w/o Valid Set │ fast_lsa_t_v2-Laptop14-yangheng/deberta-v3-base-absa-v1.1 │ [83.85579937304075] │  83.8558  │ 83.8558  │   0   │   0   │ 83.8558 │ 83.8558 │\n",
      "├────────────────────────────┼───────────────────────────────────────────────────────────┼─────────────────────┼───────────┼──────────┼───────┼───────┼─────────┼─────────┤\n",
      "│ Max-Test-F1 w/o Valid Set  │ fast_lsa_t_v2-Laptop14-yangheng/deberta-v3-base-absa-v1.1 │ [80.47214919361038] │  80.4721  │ 80.4721  │   0   │   0   │ 80.4721 │ 80.4721 │\n",
      "╘════════════════════════════╧═══════════════════════════════════════════════════════════╧═════════════════════╧═══════════╧══════════╧═══════╧═══════╧═════════╧═════════╛\n",
      "------------------------------------------------------------- https://github.com/yangheng95/metric_visualizer -------------------------------------------------------------\n",
      "\n",
      "[2025-04-27 19:27:17] (2.4.1.post1) Load sentiment classifier from trainer\n",
      "[2025-04-27 19:27:19] (2.4.1.post1) Set Model Device: cpu\n",
      "[2025-04-27 19:27:19] (2.4.1.post1) Device Name: Unknown\n",
      "2025-04-27 19:27:19,301 INFO: PyABSA version: 2.4.1.post1\n",
      "2025-04-27 19:27:19,303 INFO: Transformers version: 4.39.3\n",
      "2025-04-27 19:27:19,304 INFO: Torch version: 2.2.2+cpu+cudaNone\n",
      "2025-04-27 19:27:19,305 INFO: Device: Unknown\n",
      "2025-04-27 19:27:19,321 INFO: Searching dataset 113.Laptop14 in local disk\n",
      "2025-04-27 19:27:19,460 INFO: You can set load_aug=True in a trainer to augment your dataset (English only yet) and improve performance.\n",
      "2025-04-27 19:27:19,461 INFO: Please use a new folder to perform new text augment if the former augment in integrated_datasets\\apc_datasets\\110.SemEval\\113.laptop14 errored unexpectedly\n",
      "2025-04-27 19:27:23,964 INFO: Load dataset from integrated_datasets\\apc_datasets\\110.SemEval\\113.laptop14\\Laptops_Train.xml.seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing dataloader: 100%|██████████| 2328/2328 [00:01<00:00, 1716.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-27 19:27:25,332 INFO: Dataset Label Details: {'Negative': 870, 'Positive': 994, 'Neutral': 464, 'Sum': 2328}\n",
      "2025-04-27 19:27:26,272 INFO: train data examples:\n",
      " [{'ex_id': tensor(0), 'text_raw': 'I charge it at night and skip taking the cord with me because of the good battery life .', 'text_spc': '[CLS] I charge it at night and skip taking the cord with me because of the good battery life . [SEP] cord [SEP]', 'aspect': 'cord', 'aspect_position': tensor(0, dtype=torch.int32), 'lca_ids': tensor([0.6667, 0.7143, 0.7619, 0.8095, 0.8571, 0.9048, 0.9524, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9524, 0.9048, 0.8571, 0.8095,\n",
      "        0.7619, 0.7143, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([0.6667, 0.7143, 0.7619, 0.8095, 0.8571, 0.9048, 0.9524, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9524, 0.9048, 0.8571, 0.8095,\n",
      "        0.7619, 0.7143, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([   1,  273, 1541,  278,  288,  661,  263, 7637,  787,  262, 7443,  275,\n",
      "         351,  401,  265,  262,  397, 2643,  432,  323,    2, 7443,    2,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(1), 'cluster_ids': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    1, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0, dtype=torch.int32), 'left_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([0.6667, 0.7143, 0.7619, 0.8095, 0.8571, 0.9048, 0.9524, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9524, 0.9048, 0.8571, 0.8095,\n",
      "        0.7619, 0.7143, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([   1,  273, 1541,  278,  288,  661,  263, 7637,  787,  262, 7443,  275,\n",
      "         351,  401,  265,  262,  397, 2643,  432,  323,    2, 7443,    2,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([0.3333, 0.3810, 0.4286, 0.4762, 0.5238, 0.5714, 0.6190, 0.6667, 0.7143,\n",
      "        0.7619, 0.8095, 0.8571, 0.9048, 0.9524, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([   1,  273, 1541,  278,  288,  661,  263, 7637,  787,  262, 7443,  275,\n",
      "         351,  401,  265,  262,  397, 2643,  432,  323,    2, 2643,  432,    2,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'right_dist': tensor(7)}, {'ex_id': tensor(1), 'text_raw': 'I charge it at night and skip taking the cord with me because of the good battery life .', 'text_spc': '[CLS] I charge it at night and skip taking the cord with me because of the good battery life . [SEP] battery life [SEP]', 'aspect': 'battery life', 'aspect_position': tensor(0, dtype=torch.int32), 'lca_ids': tensor([0.3333, 0.3810, 0.4286, 0.4762, 0.5238, 0.5714, 0.6190, 0.6667, 0.7143,\n",
      "        0.7619, 0.8095, 0.8571, 0.9048, 0.9524, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([0.3333, 0.3810, 0.4286, 0.4762, 0.5238, 0.5714, 0.6190, 0.6667, 0.7143,\n",
      "        0.7619, 0.8095, 0.8571, 0.9048, 0.9524, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([   1,  273, 1541,  278,  288,  661,  263, 7637,  787,  262, 7443,  275,\n",
      "         351,  401,  265,  262,  397, 2643,  432,  323,    2, 2643,  432,    2,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(2), 'cluster_ids': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100,    2,    2, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0, dtype=torch.int32), 'left_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([0.6667, 0.7143, 0.7619, 0.8095, 0.8571, 0.9048, 0.9524, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9524, 0.9048, 0.8571, 0.8095,\n",
      "        0.7619, 0.7143, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([   1,  273, 1541,  278,  288,  661,  263, 7637,  787,  262, 7443,  275,\n",
      "         351,  401,  265,  262,  397, 2643,  432,  323,    2, 7443,    2,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'left_dist': tensor(7), 'right_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([0.3333, 0.3810, 0.4286, 0.4762, 0.5238, 0.5714, 0.6190, 0.6667, 0.7143,\n",
      "        0.7619, 0.8095, 0.8571, 0.9048, 0.9524, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([   1,  273, 1541,  278,  288,  661,  263, 7637,  787,  262, 7443,  275,\n",
      "         351,  401,  265,  262,  397, 2643,  432,  323,    2, 2643,  432,    2,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'right_dist': tensor(0)}]\n",
      "2025-04-27 19:27:26,760 INFO: Load dataset from integrated_datasets\\apc_datasets\\110.SemEval\\113.laptop14\\Laptops_Test_Gold.xml.seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing dataloader: 100%|██████████| 638/638 [00:00<00:00, 2100.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-27 19:27:27,072 INFO: Dataset Label Details: {'Negative': 128, 'Positive': 341, 'Neutral': 169, 'Sum': 638}\n",
      "2025-04-27 19:27:27,336 INFO: test data examples:\n",
      " [{'ex_id': tensor(0), 'text_raw': ' Boot time is super fast , around anywhere from 35 seconds to 1 minute .', 'text_spc': '[CLS]  Boot time is super fast , around anywhere from 35 seconds to 1 minute . [SEP] Boot time [SEP]', 'aspect': 'Boot time', 'aspect_position': tensor(0, dtype=torch.int32), 'lca_ids': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9412, 0.8824, 0.8235,\n",
      "        0.7647, 0.7059, 0.6471, 0.5882, 0.5294, 0.4706, 0.4118, 0.3529, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9412, 0.8824, 0.8235,\n",
      "        0.7647, 0.7059, 0.6471, 0.5882, 0.5294, 0.4706, 0.4118, 0.3529, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([    1, 15798,   326,   269,  1850,  1274,   366,   441,  2619,   292,\n",
      "         2453,  2490,   264,   376,  2092,   323,     2, 15798,   326,     2,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(2), 'cluster_ids': tensor([-100,    2,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0, dtype=torch.int32), 'left_lcf_cdm_vec': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9412, 0.8824, 0.8235,\n",
      "        0.7647, 0.7059, 0.6471, 0.5882, 0.5294, 0.4706, 0.4118, 0.3529, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([    1, 15798,   326,   269,  1850,  1274,   366,   441,  2619,   292,\n",
      "         2453,  2490,   264,   376,  2092,   323,     2, 15798,   326,     2,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9412, 0.8824, 0.8235,\n",
      "        0.7647, 0.7059, 0.6471, 0.5882, 0.5294, 0.4706, 0.4118, 0.3529, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([    1, 15798,   326,   269,  1850,  1274,   366,   441,  2619,   292,\n",
      "         2453,  2490,   264,   376,  2092,   323,     2, 15798,   326,     2,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'right_dist': tensor(0)}, {'ex_id': tensor(1), 'text_raw': ' tech support would not fix the problem unless I bought your plan for $ 150 plus .', 'text_spc': '[CLS]  tech support would not fix the problem unless I bought your plan for $ 150 plus . [SEP] tech support [SEP]', 'aspect': 'tech support', 'aspect_position': tensor(0, dtype=torch.int32), 'lca_ids': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9474, 0.8947, 0.8421,\n",
      "        0.7895, 0.7368, 0.6842, 0.6316, 0.5789, 0.5263, 0.4737, 0.4211, 0.3684,\n",
      "        0.3158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9474, 0.8947, 0.8421,\n",
      "        0.7895, 0.7368, 0.6842, 0.6316, 0.5789, 0.5263, 0.4737, 0.4211, 0.3684,\n",
      "        0.3158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([   1, 3539,  523,  338,  298, 2760,  262,  735, 2336,  273, 2031,  290,\n",
      "         741,  270,  419, 3732, 1783,  323,    2, 3539,  523,    2,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(0), 'cluster_ids': tensor([-100,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0, dtype=torch.int32), 'left_lcf_cdm_vec': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9474, 0.8947, 0.8421,\n",
      "        0.7895, 0.7368, 0.6842, 0.6316, 0.5789, 0.5263, 0.4737, 0.4211, 0.3684,\n",
      "        0.3158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([   1, 3539,  523,  338,  298, 2760,  262,  735, 2336,  273, 2031,  290,\n",
      "         741,  270,  419, 3732, 1783,  323,    2, 3539,  523,    2,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9474, 0.8947, 0.8421,\n",
      "        0.7895, 0.7368, 0.6842, 0.6316, 0.5789, 0.5263, 0.4737, 0.4211, 0.3684,\n",
      "        0.3158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([   1, 3539,  523,  338,  298, 2760,  262,  735, 2336,  273, 2031,  290,\n",
      "         741,  270,  419, 3732, 1783,  323,    2, 3539,  523,    2,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'right_dist': tensor(0)}]\n",
      "2025-04-27 19:27:27,338 INFO: valid data examples:\n",
      " []\n",
      "[2025-04-27 19:27:27] (2.4.1.post1) Caching dataset... please remove cached dataset if any problem happens.\n",
      "2025-04-27 19:27:32,511 INFO: Model Architecture:\n",
      " APCEnsembler(\n",
      "  (models): ModuleList(\n",
      "    (0): FAST_LSA_T_V2(\n",
      "      (bert4global): DebertaV2Model(\n",
      "        (embeddings): DebertaV2Embeddings(\n",
      "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "          (dropout): StableDropout()\n",
      "        )\n",
      "        (encoder): DebertaV2Encoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x DebertaV2Layer(\n",
      "              (attention): DebertaV2Attention(\n",
      "                (self): DisentangledSelfAttention(\n",
      "                  (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (pos_dropout): StableDropout()\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "                (output): DebertaV2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "              )\n",
      "              (intermediate): DebertaV2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DebertaV2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (rel_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (post_encoder): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (post_encoder_): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (bert_pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (CDW_LSA): LSA(\n",
      "        (encoder): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_left): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_right): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (linear_window_3h): Linear(in_features=2304, out_features=768, bias=True)\n",
      "        (linear_window_2h): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      )\n",
      "      (post_linear): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      (dense): Linear(in_features=768, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (bert): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (dense): Linear(in_features=3, out_features=3, bias=True)\n",
      ")\n",
      "2025-04-27 19:27:32,513 INFO: ABSADatasetsVersion:None\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,515 INFO: MV:<metric_visualizer.metric_visualizer.MetricVisualizer object at 0x000001E0F6C353A0>\t-->\tCalling Count:3\n",
      "2025-04-27 19:27:32,516 INFO: PyABSAVersion:2.4.1.post1\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,517 INFO: SRD:3\t-->\tCalling Count:11864\n",
      "2025-04-27 19:27:32,517 INFO: TorchVersion:2.2.2+cpu+cudaNone\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,519 INFO: TransformersVersion:4.39.3\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,520 INFO: auto_device:cpu\t-->\tCalling Count:152\n",
      "2025-04-27 19:27:32,521 INFO: batch_size:16\t-->\tCalling Count:5\n",
      "2025-04-27 19:27:32,523 INFO: cache_dataset:True\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,523 INFO: checkpoint_save_mode:0\t-->\tCalling Count:7\n",
      "2025-04-27 19:27:32,524 INFO: cross_validate_fold:-1\t-->\tCalling Count:3\n",
      "2025-04-27 19:27:32,525 INFO: dataset_file:{'train': ['integrated_datasets\\\\apc_datasets\\\\110.SemEval\\\\113.laptop14\\\\Laptops_Train.xml.seg'], 'test': ['integrated_datasets\\\\apc_datasets\\\\110.SemEval\\\\113.laptop14\\\\Laptops_Test_Gold.xml.seg'], 'valid': []}\t-->\tCalling Count:28\n",
      "2025-04-27 19:27:32,527 INFO: dataset_name:Laptop14\t-->\tCalling Count:7\n",
      "2025-04-27 19:27:32,528 INFO: dca_layer:3\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,530 INFO: dca_p:1\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,531 INFO: deep_ensemble:False\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,532 INFO: device:cpu\t-->\tCalling Count:11761\n",
      "2025-04-27 19:27:32,533 INFO: device_name:Unknown\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,534 INFO: dlcf_a:2\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,535 INFO: dropout:0.5\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,536 INFO: dynamic_truncate:True\t-->\tCalling Count:11864\n",
      "2025-04-27 19:27:32,537 INFO: embed_dim:768\t-->\tCalling Count:14\n",
      "2025-04-27 19:27:32,538 INFO: eta:1\t-->\tCalling Count:4\n",
      "2025-04-27 19:27:32,539 INFO: eta_lr:0.1\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,540 INFO: evaluate_begin:0\t-->\tCalling Count:30\n",
      "2025-04-27 19:27:32,541 INFO: from_checkpoint:None\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,542 INFO: hidden_dim:768\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,543 INFO: index_to_label:{0: 'Negative', 1: 'Neutral', 2: 'Positive', -100: ''}\t-->\tCalling Count:7\n",
      "2025-04-27 19:27:32,545 INFO: inference_model:None\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,546 INFO: initializer:xavier_uniform_\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,548 INFO: inputs_cols:['lcf_cdm_vec', 'lcf_cdw_vec', 'left_lcf_cdm_vec', 'left_lcf_cdw_vec', 'right_lcf_cdm_vec', 'right_lcf_cdw_vec', 'spc_mask_vec', 'text_indices']\t-->\tCalling Count:90296\n",
      "2025-04-27 19:27:32,550 INFO: l2reg:1e-06\t-->\tCalling Count:4\n",
      "2025-04-27 19:27:32,551 INFO: label_to_index:{'Negative': 0, 'Neutral': 1, 'Positive': 2, '-100': -100, '': -100}\t-->\tCalling Count:4\n",
      "2025-04-27 19:27:32,553 INFO: lcf:cdw\t-->\tCalling Count:6\n",
      "2025-04-27 19:27:32,554 INFO: learning_rate:2e-05\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,555 INFO: load_aug:False\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,557 INFO: log_step:5\t-->\tCalling Count:147\n",
      "2025-04-27 19:27:32,558 INFO: logger:<Logger fast_lsa_t_v2 (INFO)>\t-->\tCalling Count:36\n",
      "2025-04-27 19:27:32,559 INFO: loss:0.08270088322460652\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,560 INFO: lsa:False\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,561 INFO: max_seq_len:80\t-->\tCalling Count:71184\n",
      "2025-04-27 19:27:32,562 INFO: max_test_metrics:{'max_apc_test_acc': 0, 'max_apc_test_f1': 0}\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,563 INFO: metrics_of_this_checkpoint:{'acc': 0.8197492163009404, 'f1': 0.7822826585625221}\t-->\tCalling Count:58\n",
      "2025-04-27 19:27:32,565 INFO: model:<class 'pyabsa.tasks.AspectPolarityClassification.models.__lcf__.fast_lsa_t_v2.FAST_LSA_T_V2'>\t-->\tCalling Count:12\n",
      "2025-04-27 19:27:32,565 INFO: model_name:fast_lsa_t_v2\t-->\tCalling Count:11871\n",
      "2025-04-27 19:27:32,566 INFO: model_path_to_save:None\t-->\tCalling Count:4\n",
      "2025-04-27 19:27:32,567 INFO: num_epoch:1\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,568 INFO: optimizer:adamw\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,571 INFO: output_dim:3\t-->\tCalling Count:35\n",
      "2025-04-27 19:27:32,572 INFO: overwrite_cache:False\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,573 INFO: path_to_save:None\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,574 INFO: patience:99999\t-->\tCalling Count:6\n",
      "2025-04-27 19:27:32,576 INFO: pretrained_bert:yangheng/deberta-v3-base-absa-v1.1\t-->\tCalling Count:12\n",
      "2025-04-27 19:27:32,578 INFO: save_mode:0\t-->\tCalling Count:1\n",
      "2025-04-27 19:27:32,579 INFO: seed:52\t-->\tCalling Count:13\n",
      "2025-04-27 19:27:32,580 INFO: sigma:0.3\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,581 INFO: similarity_threshold:1\t-->\tCalling Count:4\n",
      "2025-04-27 19:27:32,582 INFO: spacy_model:en_core_web_sm\t-->\tCalling Count:9\n",
      "2025-04-27 19:27:32,583 INFO: srd_alignment:True\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,584 INFO: task_code:APC\t-->\tCalling Count:2\n",
      "2025-04-27 19:27:32,585 INFO: task_name:Aspect-based Sentiment Classification\t-->\tCalling Count:1\n",
      "2025-04-27 19:27:32,586 INFO: tokenizer:DebertaV2TokenizerFast(name_or_path='yangheng/deberta-v3-base-absa-v1.1', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t128000: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,587 INFO: use_amp:False\t-->\tCalling Count:294\n",
      "2025-04-27 19:27:32,588 INFO: use_bert_spc:True\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,590 INFO: use_syntax_based_SRD:False\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,591 INFO: warmup_step:-1\t-->\tCalling Count:147\n",
      "2025-04-27 19:27:32,592 INFO: window:lr\t-->\tCalling Count:0\n",
      "2025-04-27 19:27:32,598 INFO: ***** Running training for Aspect-based Sentiment Classification *****\n",
      "2025-04-27 19:27:32,599 INFO: Training set examples = 2328\n",
      "2025-04-27 19:27:32,599 INFO: Test set examples = 638\n",
      "2025-04-27 19:27:32,600 INFO: Total params = 197414417, Trainable params = 197414417, Non-trainable params = 0\n",
      "2025-04-27 19:27:32,601 INFO: Batch size = 16\n",
      "2025-04-27 19:27:32,602 INFO: Num steps = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Smooth Loss: 0.7124:   8%|▊         | 11/146 [04:00<49:12, 21.87s/it, Dev Acc:83.86(max:83.86) Dev F1:80.04(max:80.04)]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m     apc_config_english\u001b[38;5;241m.\u001b[39meta \u001b[38;5;241m=\u001b[39m eta\n\u001b[0;32m     34\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m APC\u001b[38;5;241m.\u001b[39mAPCDatasetList\u001b[38;5;241m.\u001b[39mLaptop14\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mAPC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPCTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# train set and test set will be automatically detected\u001b[39;49;00m\n\u001b[0;32m     37\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcheckpoint_save_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# =None to avoid save model\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mauto_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# automatic choose CUDA or CPU\u001b[39;49;00m\n\u001b[0;32m     39\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     apc_config_english\u001b[38;5;241m.\u001b[39mMV\u001b[38;5;241m.\u001b[39mnext_trial()\n\u001b[0;32m     42\u001b[0m save_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(apc_config_english\u001b[38;5;241m.\u001b[39mmodel_name, apc_config_english\u001b[38;5;241m.\u001b[39mdataset_name)\n",
      "File \u001b[1;32mc:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyabsa\\tasks\\AspectPolarityClassification\\trainer\\apc_trainer.py:69\u001b[0m, in \u001b[0;36mAPCTrainer.__init__\u001b[1;34m(self, config, dataset, from_checkpoint, checkpoint_save_mode, auto_device, path_to_save, load_aug)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_code \u001b[38;5;241m=\u001b[39m TaskCodeOption\u001b[38;5;241m.\u001b[39mAspect_Polarity_Classification\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m=\u001b[39m TaskNameOption()\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     66\u001b[0m     TaskCodeOption\u001b[38;5;241m.\u001b[39mAspect_Polarity_Classification\n\u001b[0;32m     67\u001b[0m )\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyabsa\\framework\\trainer_class\\trainer_template.py:244\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m         model_path\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_instructor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\u001b[38;5;241m.\u001b[39mrun())\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;66;03m# always return the last trained model if you don't save trained model\u001b[39;00m\n\u001b[0;32m    243\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model_class(\n\u001b[1;32m--> 244\u001b[0m             checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_instructor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m         )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m seeds\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# remove logger\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyabsa\\tasks\\AspectPolarityClassification\\instructor\\apc_instructor.py:703\u001b[0m, in \u001b[0;36mAPCTrainingInstructor.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;66;03m# Loss and Optimizer\u001b[39;00m\n\u001b[0;32m    702\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m--> 703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyabsa\\framework\\instructor_class\\instructor_template.py:368\u001b[0m, in \u001b[0;36mBaseTrainingInstructor._train\u001b[1;34m(self, criterion)\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_k_fold_train_and_evaluate(criterion)\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model if there is only one validation dataloader\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyabsa\\tasks\\AspectPolarityClassification\\instructor\\apc_instructor.py:147\u001b[0m, in \u001b[0;36mAPCTrainingInstructor._train_and_evaluate\u001b[1;34m(self, criterion)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mwarmup_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ALY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\function.py:277\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    281\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import autocuda\n",
    "import random\n",
    "\n",
    "from metric_visualizer import MetricVisualizer\n",
    "\n",
    "from pyabsa import AspectPolarityClassification as APC\n",
    "\n",
    "import warnings\n",
    "\n",
    "device = autocuda.auto_cuda()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seeds = [random.randint(0, 10000) for _ in range(3)]\n",
    "\n",
    "max_seq_lens = [60, 70, 80, 90, 100]\n",
    "\n",
    "apc_config_english = APC.APCConfigManager.get_apc_config_english()\n",
    "apc_config_english.model = APC.APCModelList.FAST_LCF_BERT\n",
    "apc_config_english.lcf = 'cdw'\n",
    "apc_config_english.max_seq_len = 80\n",
    "apc_config_english.cache_dataset = False\n",
    "apc_config_english.patience = 10\n",
    "apc_config_english.seed = seeds\n",
    "\n",
    "config = APC.APCConfigManager.get_apc_config_english()\n",
    "config.num_epoch = 1\n",
    "config.model = APC.APCModelList.FAST_LSA_T_V2\n",
    "\n",
    "MV = MetricVisualizer(name='lora_model')\n",
    "apc_config_english.MV = MV\n",
    "\n",
    "for eta in max_seq_lens:\n",
    "    apc_config_english.eta = eta\n",
    "    dataset = APC.APCDatasetList.Laptop14\n",
    "    APC.APCTrainer(config=config,\n",
    "                   dataset=dataset,  # train set and test set will be automatically detected\n",
    "                   checkpoint_save_mode=0,  # =None to avoid save model\n",
    "                   auto_device=device  # automatic choose CUDA or CPU\n",
    "                   )\n",
    "    apc_config_english.MV.next_trial()\n",
    "\n",
    "save_prefix = '{}_{}'.format(apc_config_english.model_name, apc_config_english.dataset_name)\n",
    "\n",
    "MV.summary(save_path=save_prefix, no_print=True)  # save fig_preview into .tex and .pdf format\n",
    "MV.traj_plot_by_trial(save_path=save_prefix, xlabel='', xrotation=30,\n",
    "                      minorticks_on=True)  # save fig_preview into .tex and .pdf format\n",
    "MV.violin_plot_by_trial(save_path=save_prefix, xticks=max_seq_lens,\n",
    "                        xlabel=r'$\\eta$')  # save fig_preview into .tex and .pdf format\n",
    "MV.box_plot_by_trial(save_path=save_prefix, xticks=max_seq_lens,\n",
    "                     xlabel=r'$\\eta$')  # save fig_preview into .tex and .pdf format\n",
    "MV.avg_bar_plot_by_trial(save_path=save_prefix, xticks=max_seq_lens,\n",
    "                         xlabel=r'$\\eta$')  # save fig_preview into .tex and .pdf format\n",
    "MV.sum_bar_plot_by_trial(save_path=save_prefix, xticks=max_seq_lens,\n",
    "                         xlabel=r'$\\eta$')  # save fig_preview into .tex and .pdf format\n",
    "MV.scott_knott_plot(save_path=save_prefix, minorticks_on=False, xticks=max_seq_lens,\n",
    "                    xlabel=r'$\\eta$')  # save fig_preview into .tex and .pdf format\n",
    "\n",
    "# print(MV.rank_test_by_trail('trial0'))  # save fig_preview into .tex and .pdf format\n",
    "# print(MV.rank_test_by_metric('metric1'))  # save fig_preview into .tex and .pdf format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-27 19:31:43] (2.4.1.post1) [ASP] tag is detected, please use [B-ASP] and [E-ASP] to annotate aspect terms.\n",
      "[2025-04-27 19:31:43] (2.4.1.post1) [ASP] tag is detected, please use [B-ASP] and [E-ASP] to annotate aspect terms.\n",
      "[2025-04-27 19:31:45] (2.4.1.post1) Example 0: The < food :Positive(confidence:0.998, ref:-100)> is good but the< service :Negative(confidence:0.988, ref:-100)>is bad .\n"
     ]
    }
   ],
   "source": [
    "lora_output = lora_model.predict(\n",
    "    text=apc_inputs,\n",
    "    print_result=True,\n",
    "    ignore_error=True,\n",
    "    eval_batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: The  food  is good but the service is bad .\n",
      "\n",
      "- Aspect: \"food\" | Sentiment: Positive | Confidence: 99.81%\n",
      "- Aspect: \"service\" | Sentiment: Negative | Confidence: 98.82%\n"
     ]
    }
   ],
   "source": [
    "def format_atepc_output(results):\n",
    "    for item in results:\n",
    "        print(f\"\\nSentence: {item['text'].strip()}\\n\")\n",
    "        for aspect, sentiment, confidence in zip(item['aspect'], item['sentiment'], item['confidence']):\n",
    "            aspect_clean = aspect.strip()\n",
    "            confidence_percent = round(confidence * 100, 2)\n",
    "            print(f\"- Aspect: \\\"{aspect_clean}\\\" | Sentiment: {sentiment} | Confidence: {confidence_percent}%\")\n",
    "\n",
    "format_atepc_output(apc_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The  food  is good but the service is bad .\n",
      "\n",
      "Output: (food: \"positive\", service: \"negative\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_atepc_output_custom(results):\n",
    "    for item in results:\n",
    "        aspects_info = []\n",
    "        for aspect, sentiment in zip(item['aspect'], item['sentiment']):\n",
    "            aspect_clean = aspect.strip()\n",
    "            sentiment_clean = sentiment.strip()\n",
    "            aspects_info.append(f'{aspect_clean}: \"{sentiment_clean.lower()}\"')\n",
    "        \n",
    "        aspects_str = ', '.join(aspects_info)\n",
    "        print(f\"Input: {item['text'].strip()}\\n\")\n",
    "        print(f\"Output: ({aspects_str})\\n\")\n",
    "\n",
    "format_atepc_output_custom(apc_output)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7261517,
     "sourceId": 11581176,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
